Blog Post \ref{whyq} introduced the concept of evenly spread points, which are commonly referred to as \emph{low discrepancy} (LD) points.  This is in contrast to independent and identically distributed (IID) points.  

Consider two sequences,
\begin{equation}
    \vT_1, \vT_2, \ldots \overset{\text{IID}}{\sim} \cu[0,1]^d, \qquad \vX_1, \vX_2, \ldots \overset{\text{LD}}{\sim} \cu[0,1]^d.
\end{equation}
\FredComment{Aleksei, please add two plots here.}
Both sequences are expected to look like points spread uniformly over the unit cube, $[0,1]^d$.  
The first sequence must be random, or as random looking as our (deterministic) random number generators can make it.  Since the points are independent, the location of any $\vT_i$ has no bearing on the location of any other $\vT_j$.  Removing a point at random does not affect the IID property.

The second sequence may be random or deterministic.  Let $F_{\{\vX_1, \ldots, \vX_n\}}$ denote the empirical distribution function of the first $n$ points of this sequence, i.e., the probability distribution that assigns a probability of $1/n$ to each location $\vX_i$.  For $\vX_1, \vX_2, \ldots$ to be LD, $F_{\{\vX_1, \ldots, \vX_n\}}$ should be close to the uniform probability distribution, $F_{\text{unif}}: \vx \mapsto x_1 \cdots x_d$.

``Close'' implies that we can measure how far apart two distributions are, and we call this measure the \emph{discrepancy}. Just like beauty is in the eye of the beholder, so there are different measurements of discrepancy.  They tend to take the form of a the distance between the empirical distribution of the point set, $F_{\{\vX_1, \ldots, \vX_n\}}$, and the target measure, $F_{\text{unif}}$. An example is the star discrepancy:
\begin{equation*}
        \disc(\{\vX_1, \ldots, \vX_n\}) : = \sup_{\vx \in [0,1]^d}  \abs{F_{\text{unif}} (\vx) - F_{\{\vX_1, \ldots, \vX_n\}}(\vx)},
\end{equation*}
which is known in the statistics literature as a Kolomogorov-Smirnov goodness-of-fit statistic.  This discrepancy compares is the maximum absolute difference between the volume of the box $[\vzero,\vx]^d$ and the proportion of the points $\{\vX_1, \ldots, \vX_n\}$ that lie in that box.  Ideally, these should be the same, but practically they will be at least a bit different.

The computational cost of evaluating the star discrepancy can be rather large, typically at least $\Order(n^d)$ operations.  A family of  computationally cheaper discrepancies is defined in terms of kernel, $K: [0,1]^d \times [0,1]^d \to \reals$, which satisfies two crucial properties:
\begin{align*}
    \text{Symmetry:} \quad& K(\vt,\vx) = K(\vx,\vt) \qquad \forall \vt, \vx \in [0,1]^d, \\
    \text{Positive Definiteness:} \quad& \vc^T \mK \vc > 0, \text{ where } \mK =\bigl(K(\vx_i, \vx_j) \bigr)_{i,j=1}^n, \\
    & \qquad \qquad \forall \vc \ne \vzero,  \text{ distinct } \vx_1, \vx_2, \ldots \in [0,1]^d.
\end{align*}
For such a kernel, we may define a discrepancy as 
\begin{align*}
        \MoveEqLeft\disc(\{\vX_1, \ldots, \vX_n\}) \\
        & : = \int_{[0,1]^d \times [0,1]^d} K(\vt, \vx) \, \dif (F_{\text{unif}} - F_{\{\vX_1, \ldots, \vX_n\}})(\vt) \, \dif (F_{\text{unif}} - F_{\{\vX_1, \ldots, \vX_n\}}) (\vx) \\
        & = \int_{[0,1]^d \times [0,1]^d} K(\vt, \vx) \, \dif \vt  \dif \vx
        - \frac 2n \sum_{i=1}^n  \int_{[0,1]^d} K(\vx_i,\vx) \, \dif \vx \\
        & \qquad \qquad + \frac{1}{n^2} \sum_{i,j=1}^n K(\vx_i, \vx_j)
\end{align*}

For example, the centered $L^2$-discrepancy \cite{Hic97a} is defined in terms of the kernel
\begin{equation*}
    K(\vt,\vx) = \prod_{k=1}^d \left[1 + \frac 12 \abs{t_k - 1/2} + \frac 12 \abs{x_k - 1/2} = \frac 12 \abs{t_k - x_k} \right].
\end{equation*}
After straightforward calculations it becomes
\begin{multline*}
        \disc(\{\vX_1, \ldots, \vX_n\})  = \left(\frac{13}{12} \right)^d 
        - \frac 2n \sum_{i=1}^n  \prod_{k=1}^d \left(1 + \frac 12 \abs{x_k - 1/2} - \frac 12 \abs{x_k - 1/2}^2 \right)\\
       + \frac{1}{n^2} \sum_{i,j=1}^n\prod_{k=1}^d \left[1 + \frac 12 \abs{x_{ik} - 1/2} + \frac 12 \abs{x_{jk} - 1/2} = \frac 12 \abs{x_{ik} - x_{jk}} \right].
\end{multline*}
This discrepancy only requires $\Order(dn^2)$ operations to evaluate.

LD sequences have discrepancies of $\Order(n^{-1 +\epsilon})$ for the discrepancies illustrated above.  IID sequences have root mean square discrepancies of $\Order(n^{-1/2})$.  This difference is asymptotic order can translate into orders of magnitude improvements in the accuracy of numerical solutions.

For high dimensional problems introducing coordinate weights into the 



