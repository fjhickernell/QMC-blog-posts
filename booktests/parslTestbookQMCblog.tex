\begin{comment}
\title{parslTestbookQMCblog}
\author{Joshua Jay Herman (QMC Development Team),  Brandon Sharp, and Sou-Cheng Choi (Illinois Tech and SouLab LLC)}
\date{September 2025}

\maketitle
Accelerating QMCpy Notebook Tests with Parsl
\end{comment}

\subsection{Introduction}

Notebook regression testing \scnote{is it regression testing or unit testing?} ensures that interactive examples and analyses
remain correct and reproducible, catching regressions introduced by changes in
code, dependencies, or execution environments. For QMCPy \cite{QMCPy2020a}, this
process is both massively parallel and resource-intensive due to the number and
complexity of its notebooks.

This blog post summarizes our work on accelerating notebook regression testing
(using Testbook-based tests, which can be viewed as notebook-level unit tests)
as presented in our ParslFest 2025 talk \cite{parslfest2025}, and outlines
directions for further development.

The slides accompanying the presentation are available at:
\href{https://github.com/QMCSoftware/QMCSoftware/raw/refs/heads/parsl_presentation/demos/talk_paper_demos/Parslfest_2025/Parsl%20Testbook%20Speedup.pptx}{Parsl Testbook Speedup}.
\scnote{Link broken.}

\subsection{Methodology}

Our choice to adopt Testbook \cite{testbook2021} is motivated by its ability to
execute Jupyter notebooks directly within a test environment, enabling
fine-grained validation of both code cells and notebook state. Testbook also
integrates cleanly with our existing testing directory structure, where other
unit tests are organized without requiring full notebook execution. This
preserves modularity, simplifies debugging, and avoids unnecessary duplication
of logic.

To support scalable notebook testing, we developed a lightweight yet flexible
test harness that enables Parsl \cite{parsl2019} to orchestrate Testbook-based
unit tests. By treating each notebook test as an independent Parsl app, the
harness realizes an embarrassingly parallel workflow suitable for local
multiprocessing, HPC schedulers, or cloud environments.

The harness coordinates three primary components to achieve reproducible,
high-throughput notebook testing:

\begin{itemize}
    \item \textbf{Continuous Integration (CI):} A GitHub Actions workflow
    prepares the execution environment (Conda environment creation, minimal
    \LaTeX{} installation, optional swap configuration), installs project
    dependencies, and triggers the appropriate test targets (e.g.,
    \texttt{make booktests\_parallel\_no\_docker}). This ensures consistent,
    version-controlled execution across platforms.

    \item \textbf{Parsl controller and workers:} Parsl provisions local or
    remote executors---processes, threads, or cluster jobs---and schedules
    notebook tests as independent tasks. This enables parallel execution with
    configurable concurrency limits, resource profiles, and executor backends.

    \item \textbf{Testbook runner and artifact collection:} Each worker
    executes its assigned notebook tests through Testbook. Outputs, execution
    logs, error traces, generated figures, and notebook artifacts with executed
    cells are returned to the Parsl controller and uploaded by CI for
    inspection, provenance tracking, and debugging.
\end{itemize}

Key features of the harness include pinned Conda environment specifications for
reproducibility, customizable Parsl executors (local, HPC, or cloud), timeout
and retry policies for handling flaky or long-running tests, and centralized
logging to streamline diagnosis of failures. Together, these components provide
a robust framework for scalable, automated validation of computational
notebooks.


\subsection{Results}


To establish a performance baseline, we first measured the wall‐clock time
required to execute a representative subset of demo notebooks sequentially.
After extending test coverage to include syntax-validation checks and
additional notebooks, we repeated the experiment under the parallel
Testbook–Parsl workflow. Across these configurations, we observed a consistent
\textbf{3.0-fold speedup}, demonstrating that notebook-based tests parallelize
cleanly and benefit substantially from concurrent execution. The overall trend
is illustrated in Figure~\ref{fig:parsl_speedup}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{booktests/parsl_speedup.png}
    \caption{Speedup achieved by running Testbook-based notebook tests under
    Parsl with four workers compared to sequential execution.}
    \label{fig:parsl_speedup}
\end{figure}

All tests were executed on a Linux system (AMD64 architecture) with 16~CPU
cores. When run in continuous integration, the workflow executes the same test
suite on the GitHub Actions \texttt{ubuntu-latest} runner. Users may reproduce
the parallel Testbook workflow locally by running:
\begin{verbatim}
make testbook
\end{verbatim}

\medskip
\noindent
Table~\ref{tab:coverage_summary} summarizes the scope of the tested
notebooks and their corresponding generated test files.

\begin{table}[h]
\centering
\caption{Coverage summary for the QMCPy Testbook suite.}
\label{tab:coverage_summary}
\begin{tabular}{l r l}
\hline
\textbf{Item} & \textbf{Count} & \textbf{Notes} \\
\hline
Demo notebooks in \texttt{demos/} & 33 &
All notebooks in \texttt{demos/} (including subfolders) \\
Generated test files in \texttt{test/booktests/tb\_*.py} & 32 &
Each \texttt{tb\_*.py} tests a single demo notebook \\
Notebooks executed by booktests workflow & 32 &
Workflow runs the \texttt{make} targets and executes \\ 
& &  all generated test files (one per notebook) \\
\hline
\end{tabular}
\end{table}

\paragraph{Runner configuration notes.}
\begin{itemize}
    \item The GitHub \texttt{ubuntu-latest} runner typically provides
    2~virtual CPUs per job.
    \item The workflow allocates a 12~GB swap file to mitigate transient
    memory spikes during notebook execution and reduce the likelihood of
    out-of-memory failures.
\end{itemize}

\subsection*{CI Tests (from \texttt{.github/workflows/booktests.yml})}

This section summarizes how the CI workflow executes the notebook-based tests
and how to inspect or extend the run.

\begin{itemize}
    \item The workflow checks out the repository, sets up Miniconda, and installs
    project test extras (\texttt{pip install -e .[test]}) plus additional optional
    extras (\texttt{test\_torch}, \texttt{test\_gpytorch}, etc.).
    
    \item It creates a 12~GB swap file early in the job to reduce out-of-memory
    failures for memory-heavy notebooks.
    
    \item The workflow ensures that \texttt{test/booktests/tb\_*.py} files exist
    by running \texttt{make check\_booktests} and \texttt{make generate\_booktests}.
    
    \item The final step runs the test target, e.g.,
    \texttt{make booktests\_parallel\_no\_docker} (or
    \texttt{make booktests\_no\_docker} for sequential execution). This target
    executes the generated \texttt{tb\_*.py} tests (one per notebook) using
    \texttt{pytest}/\texttt{testbook}.
\end{itemize}

\paragraph{Debugging and diagnostics.}
\begin{itemize}
    \item Setting \texttt{ACTIONS\_STEP\_DEBUG: true} at the job level enables
    more verbose step logs.
    
    \item Add quick diagnostic lines in the workflow to show runner resources:
\begin{verbatim}
- name: Show runner resources
  run: |
    echo "CPUs: $(nproc)"
    free -h
    df -h
\end{verbatim}

    \item To capture timings and logs, wrap the \texttt{make} invocation with
    \texttt{/usr/bin/time -v} and upload the output using
    \texttt{actions/upload-artifact@v4}.
\end{itemize}

\paragraph{Parallelism and resource tuning.}
\begin{itemize}
    \item The GitHub runner provides 2~vCPUs by default; the Parsl demo notebook
    may request more workers (e.g., \texttt{max\_workers = 8}). Consider adapting
    the notebook to:
\begin{verbatim}
max_workers = min(8, os.cpu_count() or 1)
\end{verbatim}
    so that it respects available runner resources.
    
    \item If CI encounters memory pressure, run the sequential
    \texttt{booktests\_no\_docker} target or constrain \texttt{-j} parallelism
    in the \texttt{Makefile} target.
\end{itemize}

\paragraph{Extensibility.}
\begin{itemize}
    \item Cache package installs using \texttt{actions/cache} to speed up CI runs.
    \item Upload per-notebook HTML or output artifacts for post-failure inspection.
    \item Add a lightweight timing reporter (CSV) in \texttt{test/booktests/}
    and upload it from CI to track per-notebook runtimes over time.
\end{itemize}

\subsection{Further Work}

Due to the above results, this indicates that we should extend our testing to doctest and `pytest` in Parsl.

Now, because many people have multicore processors, we can increase individual productivity so that our tests can demonstrate that no regressions have been introduced.

Furthermore, regarding feedback on the presentation to the ParslFest participants, the system is quite general. This suggests a distributed test system could benefit Parsl users by enabling them to distribute their own test workloads.

Finally, we could expand our work to encompass Python doctests, as well as unit testing using pytest or unittest, in addition to testing Jupyter notebooks.